# Empirica Architecture

**Clean, modular biomedical research assistant with RAG-enhanced knowledge graphs**

## ğŸ—ï¸ System Overview

Empirica is a full-stack application that processes biomedical research papers, extracts knowledge graphs, and provides AI-powered insights through a RAG (Retrieval-Augmented Generation) system.

### Core Features
- âœ… **Per-PDF Graph System**: Each PDF gets its own knowledge graph
- âœ… **Dynamic Graph Merging**: Combine multiple PDF graphs by selection
- âœ… **RAG Integration**: Document chunking, indexing, and semantic retrieval
- âœ… **LLM-Powered Insights**: Hypothesis generation and conversational AI via Lava + Anthropic Claude
- âœ… **Import/Export**: Full project state including RAG indices
- âœ… **Multi-user Support**: OAuth authentication with Google

---

## ğŸ“ Project Structure

```
calhacks/
â”œâ”€â”€ backend/                    # FastAPI Python backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration (Lava keys, etc.)
â”‚   â”‚   â”œâ”€â”€ main.py            # API endpoints
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py    # SQLAlchemy models (per-PDF graphs)
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py     # Pydantic request/response schemas
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ pdf_processor.py           # PDF text extraction
â”‚   â”‚       â”œâ”€â”€ ner_service.py             # Named Entity Recognition
â”‚   â”‚       â”œâ”€â”€ relationship_extractor.py  # Entity relationship extraction
â”‚   â”‚       â”œâ”€â”€ graph_builder.py           # NetworkX graph construction
â”‚   â”‚       â”œâ”€â”€ document_chunker.py        # Smart PDF chunking for RAG
â”‚   â”‚       â”œâ”€â”€ rag_service.py             # RAG indexing & retrieval
â”‚   â”‚       â”œâ”€â”€ content_insight_agent.py   # Insight generation
â”‚   â”‚       â”œâ”€â”€ graph_agent.py             # Conversational AI
â”‚   â”‚       â”œâ”€â”€ llm_service.py             # LLM abstraction layer
â”‚   â”‚       â”œâ”€â”€ lava_service.py            # Lava Payments integration
â”‚   â”‚       â”œâ”€â”€ pubmed_service.py          # PubMed API integration
â”‚   â”‚       â””â”€â”€ ctgov_service.py           # ClinicalTrials.gov API
â”‚   â”œâ”€â”€ uploads/               # PDF storage & RAG indices
â”‚   â”œâ”€â”€ synapse_mapper.db      # SQLite database
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ .env                   # Environment variables (Lava keys)
â”‚
â”œâ”€â”€ frontend/                  # React + TypeScript + Vite
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx           # Main app component
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx            # Main navigation & controls
â”‚   â”‚   â”‚   â”œâ”€â”€ UploadPanel.tsx        # PDF upload interface
â”‚   â”‚   â”‚   â”œâ”€â”€ ProjectSelection.tsx   # Project management
â”‚   â”‚   â”‚   â”œâ”€â”€ PDFSelector.tsx        # PDF selection & management
â”‚   â”‚   â”‚   â”œâ”€â”€ ForceGraph2DView.tsx   # 2D graph visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ ForceGraph3DView.tsx   # 3D graph visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ NodeDetails.tsx        # Entity details panel
â”‚   â”‚   â”‚   â”œâ”€â”€ Analytics.tsx          # Graph analytics
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx          # AI chat interface
â”‚   â”‚   â”‚   â””â”€â”€ ExportMenu.tsx         # Import/export controls
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts        # Backend API client
â”‚   â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â”‚   â””â”€â”€ useStore.ts   # Zustand global state
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”‚       â””â”€â”€ index.ts      # TypeScript type definitions
â”‚   â””â”€â”€ package.json          # Node dependencies
â”‚
â””â”€â”€ Documentation
    â”œâ”€â”€ README.md             # Main project documentation
    â”œâ”€â”€ ARCHITECTURE.md       # This file
    â”œâ”€â”€ RAG_SYSTEM.md         # RAG implementation details
    â””â”€â”€ LAVA_SETUP.md         # Lava Payments setup guide
```

---

## ğŸ”„ Data Flow

### 1. PDF Upload & Processing

```
User uploads PDFs
    â†“
FastAPI receives files â†’ Saves to uploads/
    â†“
Background task (process_pdfs_background):
    For each PDF:
        1. Extract text (PDFProcessor)
        2. Extract entities (NERService)
        3. Chunk document (DocumentChunker) â† RAG
        4. Index chunks (RAGService) â† RAG
        5. Extract relationships (RelationshipExtractor)
        6. Build graph (GraphBuilder)
        7. Save to database (PDFGraphNode, PDFGraphEdge)
    â†“
Save RAG index to uploads/{project_id}_rag_index.pkl
    â†“
Return project_id to frontend
```

### 2. Graph Visualization

```
User selects project
    â†“
Frontend calls /api/projects/{id}/graph
    â†“
Backend:
    1. Query selected PDFs
    2. Load PDFGraphNodes & PDFGraphEdges
    3. Merge graphs (combine nodes, aggregate edges)
    4. Return merged graph JSON
    â†“
Frontend renders with ForceGraph2D/3D
```

### 3. Hypothesis Generation (RAG-Enhanced)

```
User clicks "Generate Hypotheses"
    â†“
Frontend sends graph + project_id to /api/hypotheses
    â†“
Backend:
    1. Build NetworkX graph from nodes/edges
    2. Load RAG index (uploads/{project_id}_rag_index.pkl)
    3. Set graph context in RAGService
    4. Retrieve top entities from graph
    5. RAG retrieval: semantic search + graph context
    6. Build RAG-enhanced prompt
    7. Send to LLM (via LavaService â†’ Anthropic Claude)
    8. Return insights
    â†“
Frontend displays in sidebar
```

### 4. Chat (RAG-Enhanced)

```
User types message in chat
    â†“
Frontend sends message + graph + project_id to /api/chat
    â†“
Backend:
    1. Load RAG index
    2. Extract entities from user message
    3. RAG retrieval: query + entities + graph context
    4. Build RAG-enhanced prompt
    5. Send to LLM via GraphConversationalAgent
    6. Return response with citations
    â†“
Frontend displays in ChatPanel
```

### 5. Import/Export

**Export:**
```
User clicks Export
    â†“
Backend:
    1. Fetch project metadata
    2. Fetch all PDFs with their graphs
    3. Load RAG index
    4. Serialize RAG index to base64
    5. Build JSON with:
        - Project info
        - PDF metadata
        - Individual PDF graphs
        - RAG index (in settings)
    â†“
Frontend downloads JSON file
```

**Import:**
```
User uploads JSON file
    â†“
Backend:
    1. Parse JSON
    2. Create new project
    3. For each PDF:
        - Copy PDF file
        - Create Document record
        - Create PDFGraphNodes & PDFGraphEdges
    4. Restore RAG index from base64
    5. Save RAG index to uploads/
    â†“
Return new project_id
```

---

## ğŸ—„ï¸ Database Schema

### Core Tables

**`projects`**
- `id` (PK): UUID
- `name`: Project name
- `description`: Optional description
- `user_id`: Owner (FK to users)
- `created_at`: Timestamp

**`documents`** (PDFs)
- `id` (PK): UUID
- `project_id` (FK): Parent project
- `filename`: Original filename for display
- `file_path`: Full path to PDF file
- `processed`: Status (-1=error, 0=pending, 1=done)
- `selected`: Boolean (1=included in merged graph)
- `original_name`: Original upload filename
- `created_at`: Timestamp

**`pdf_graph_nodes`** (Per-PDF entities)
- `id` (PK): Auto-increment
- `document_id` (FK): Parent PDF
- `entity_id`: Entity name (e.g., "BDNF")
- `entity_type`: Type (GENE, PROTEIN, DISEASE, etc.)
- `count`: Occurrence count in PDF
- `degree`: Graph centrality

**`pdf_graph_edges`** (Per-PDF relationships)
- `id` (PK): Auto-increment
- `document_id` (FK): Parent PDF
- `source`: Source entity ID
- `target`: Target entity ID
- `weight`: Relationship strength
- `relationship_type`: Type (CO_OCCURRENCE, REGULATES, etc.)
- `evidence`: JSON array of evidence sentences

**`users`** (OAuth)
- `id` (PK): UUID
- `email`: User email
- `name`: Display name
- `google_id`: Google OAuth ID
- `created_at`: Timestamp

---

## ğŸ§  RAG System Architecture

### Components

**1. DocumentChunker** (`document_chunker.py`)
- Sentence-aware chunking (respects sentence boundaries)
- Configurable chunk size (default: 500 chars) & overlap (default: 100 chars)
- Entity tracking: links entities to chunks they appear in
- Page number preservation

**2. RAGService** (`rag_service.py`)
- **Indexing**: Stores text chunks with embeddings (via sentence-transformers)
- **Graph Context**: Links chunks to knowledge graph entities
- **Hybrid Retrieval**: Combines semantic similarity + graph connectivity
- **Context Assembly**: Builds rich prompts with relevant chunks + graph relationships
- **Persistence**: Saves/loads indices with pickle

**3. Integration Points**
- **PDF Processing**: Chunks indexed during upload
- **Hypothesis Generation**: RAG retrieves context for LLM prompts
- **Chat**: RAG retrieves context for user queries
- **Import/Export**: RAG index serialized in project JSON

### RAG Workflow

```
Document â†’ Chunk â†’ Embed â†’ Index
                              â†“
User Query â†’ Semantic Search + Graph Filter â†’ Top K Chunks
                                                    â†“
                                        Build Prompt with Context
                                                    â†“
                                                  LLM
```

**See `RAG_SYSTEM.md` for detailed implementation.**

---

## ğŸ¤– LLM Integration

### Architecture

**LLMService** (`llm_service.py`)
- Abstraction layer for LLM providers
- Currently supports: **Anthropic Claude** (via Lava)
- Methods:
  - `extract_relationships()`: Extract entity relationships from text
  - `generate_insights()`: Generate research hypotheses
  - `chat()`: Conversational responses

**LavaService** (`lava_service.py`)
- Handles Lava Payments authentication & billing
- Forwards requests to Anthropic API
- Tracks usage via Lava metadata
- Base64-encoded auth token with:
  - `secret_key`: Lava API key
  - `connection_secret`: Lava connection
  - `product_secret`: Lava product

### Configuration

**Environment Variables** (`.env`):
```bash
LAVA_SECRET_KEY=aks_live_...
LAVA_CONNECTION_SECRET=cons_live_...
LAVA_PRODUCT_SECRET=ps_live_...
ENABLE_LAVA=true
```

**See `LAVA_SETUP.md` for setup instructions.**

---

## ğŸ¨ Frontend Architecture

### State Management (Zustand)

**`useStore.ts`** - Global state:
```typescript
{
  // Graph data
  graphData: { nodes: [], edges: [] },
  selectedNode: Node | null,
  
  // Project management
  currentProject: ProjectInfo | null,
  pdfs: PDFMetadata[],
  selectedPdfIds: Set<string>,
  
  // UI state
  viewMode: '2d' | '3d',
  hypotheses: Hypothesis[],
  chatMessages: Message[],
  
  // Actions
  setGraphData, selectNode, loadProject,
  updatePdfSelection, addPdfsToProject, ...
}
```

### Key Components

**Sidebar** - Main control panel
- Project selection
- PDF management (PDFSelector)
- Hypothesis generation
- Analytics
- Import/Export

**ForceGraph2D/3D** - Graph visualization
- Force-directed layout
- Node coloring by entity type
- Interactive zoom/pan
- Click to select nodes

**ChatPanel** - AI chat interface
- Conversational AI with graph context
- RAG-enhanced responses
- Citation support

**NodeDetails** - Entity information
- Entity type, occurrence count
- Connected entities
- Evidence sentences

---

## ğŸ”Œ API Endpoints

### Projects
- `POST /api/projects` - Create project & upload PDFs
- `GET /api/projects` - List user's projects
- `GET /api/projects/{id}` - Get project details
- `GET /api/projects/{id}/pdfs` - List project PDFs
- `GET /api/projects/{id}/graph` - Get merged graph
- `DELETE /api/projects/{id}` - Delete project

### PDFs
- `POST /api/projects/{id}/pdfs` - Add PDFs to project
- `DELETE /api/projects/{project_id}/pdfs/{pdf_id}` - Remove PDF
- `POST /api/projects/{id}/pdfs/selection` - Update PDF selection

### AI Features
- `POST /api/hypotheses` - Generate insights (RAG-enhanced)
- `POST /api/chat` - Chat with graph (RAG-enhanced)

### Import/Export
- `POST /api/export` - Export project JSON
- `POST /api/import` - Import project JSON

### External Data
- `GET /api/pubmed/search` - Search PubMed
- `GET /api/clinicaltrials/search` - Search ClinicalTrials.gov

### Processing
- `GET /api/processing/{job_id}` - Check processing status

---

## ğŸš€ Deployment

### Backend Setup

```bash
cd backend
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Create .env file
cat > .env << EOF
LAVA_SECRET_KEY=aks_live_...
LAVA_CONNECTION_SECRET=cons_live_...
LAVA_PRODUCT_SECRET=ps_live_...
ENABLE_LAVA=true
EOF

# Run server
uvicorn app.main:app --reload
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

### Production Considerations

**Backend:**
- Use PostgreSQL instead of SQLite
- Add Redis for caching
- Deploy with Gunicorn + Nginx
- Set up CORS properly
- Use environment-specific configs

**Frontend:**
- Build with `npm run build`
- Serve with Nginx or CDN
- Enable gzip compression
- Add error tracking (Sentry)

**RAG:**
- Consider vector database (Pinecone, Weaviate) for large scale
- Implement chunking strategies per document type
- Add embedding caching

---

## ğŸ§ª Testing

### Backend Tests
```bash
cd backend
pytest tests/
```

### Frontend Tests
```bash
cd frontend
npm test
```

### LLM Connection Test
```bash
cd backend
source venv/bin/activate
python -c "from app.services import LavaService; import asyncio; asyncio.run(LavaService().test_connection())"
```

---

## ğŸ“Š Performance Optimization

### Current Optimizations
- âœ… Background PDF processing (FastAPI BackgroundTasks)
- âœ… Lazy RAG index loading
- âœ… Graph merging on-demand
- âœ… Frontend state caching (Zustand)

### Future Improvements
- [ ] Implement pagination for large graphs
- [ ] Add graph node clustering for visualization
- [ ] Cache LLM responses
- [ ] Implement incremental RAG updates
- [ ] Add WebSocket for real-time processing updates

---

## ğŸ”’ Security

### Current Measures
- âœ… OAuth authentication (Google)
- âœ… User-scoped data access
- âœ… SQL injection prevention (SQLAlchemy ORM)
- âœ… CORS configuration
- âœ… API key security (environment variables)

### Recommendations
- [ ] Add rate limiting
- [ ] Implement API key rotation
- [ ] Add request validation middleware
- [ ] Enable HTTPS in production
- [ ] Add audit logging

---

## ğŸ“š Key Technologies

**Backend:**
- FastAPI (Python web framework)
- SQLAlchemy (ORM)
- NetworkX (graph analysis)
- spaCy (NER)
- sentence-transformers (embeddings)
- PyMuPDF (PDF processing)
- httpx (async HTTP)

**Frontend:**
- React 18 (UI framework)
- TypeScript (type safety)
- Vite (build tool)
- Zustand (state management)
- react-force-graph (visualization)
- Tailwind CSS (styling)

**AI/ML:**
- Anthropic Claude (LLM)
- Lava Payments (usage billing)
- sentence-transformers (RAG embeddings)

---

## ğŸ¤ Contributing

### Code Style
- **Backend**: Follow PEP 8, use type hints
- **Frontend**: Use TypeScript strict mode, follow React best practices

### Git Workflow
1. Create feature branch from `main`
2. Make changes with clear commit messages
3. Test thoroughly
4. Submit pull request

---

## ğŸ“ License

[Add license information]

---

## ğŸ†˜ Support

For issues or questions:
- Check `README.md` for setup instructions
- Review `RAG_SYSTEM.md` for RAG details
- See `LAVA_SETUP.md` for LLM configuration

---

**Last Updated:** October 2025
**Version:** 2.0 (RAG-Enhanced)

